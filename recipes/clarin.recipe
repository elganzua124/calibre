from __future__ import unicode_literals
__license__   = 'GPL v3'
__copyright__ = '2008-2012, Darko Miletic <darko.miletic at gmail.com>'
'''
clarin.com
'''

from calibre.web.feeds.news import BasicNewsRecipe

class Clarin(BasicNewsRecipe):
    title                 = 'Clarín'
    __author__            = 'Francisco Herrera'
    description           = 'Clarin.com. Noticias de la Argentina y el mundo. Información actualizada las 24 horas y en español. Informate ya'
    publisher             = 'Grupo Clarin'
    category              = 'news, politics, Argentina'
    oldest_article        = 1
    max_articles_per_feed = 50
    use_embedded_content  = False
    no_stylesheets        = True
    encoding              = 'utf8'
    delay                 = 1
    language              = 'es_AR'
    publication_type      = 'newspaper'
    INDEX                 = 'http://www.clarin.com'
    masthead_url          = 'http://www.clarin.com/static/CLAClarin/images/logo-clarin-print.jpg'
    extra_css             = """
                               body{font-family: Arial,Helvetica,sans-serif}
                               .columnista {
font-size: small;

}
                            """

    ignore_duplicate_articles = {'title', 'url'} # mucho redireccionamiento entre clarin.com y clarin.ieco.com

    conversion_options = {
                          'comment'  : description
                        , 'tags'     : category
                        , 'publisher': publisher
                        , 'language' : language
                        }

    remove_tags_after       = [
		dict(name='div', attrs={'class':'nota'})
	]
    remove_attributes = ['lang']

    feeds = [
               (u'Página principal' , u'http://www.clarin.com/rss/')
              ,(u'Política'         , u'http://www.clarin.com/rss/politica/')
              ,(u'Sociedad'         , u'http://www.clarin.com/rss/sociedad/')

              ,(u'Deportes'         , u'http://www.clarin.com/rss/deportes/')
              ,(u'Mundo'            , u'http://www.clarin.com/rss/mundo/')
              ,(u'iEco'             , u'http://www.clarin.com/rss/ieco/') 
              ,(u'ExtraShow'        , u'http://www.clarin.com/rss/extrashow/')
              
              ,(u'Ciudades'         , u'http://www.clarin.com/rss/ciudades/')
              ,(u'Policiales'       , u'http://www.clarin.com/rss/policiales/')
              ,(u'Tecnología'       , u'http://next.clarin.com/rss/')
              ,(u'Rural'            , u'http://www.clarin.com/rss/rural/')
              ,(u'Educación'        , u'http://www.clarin.com/rss/educacion/')
           ]

    def print_version(self, url):
        return self.browser.open_novisit(url).geturl()

    def get_cover_url(self):
        from datetime import datetime, timedelta

        br = self.cloned_browser
        dat = datetime.now()
        for x in (0,1):
            stg = dat.strftime("%Y%m%d")
            cover_url = "http://tapas.clarin.com/tapa/{}/{}/{}/{}_thumb.jpg".format(stg[:4],stg[4:6],stg[6:8],stg)
            try:
                br.open(cover_url)
                break
            except Exception as e:
                if hasattr(e, 'getcode') and e.getcode() == 404:
                    dat = dat - timedelta(days=1)
                cover_url = None
        return cover_url

    #def is_link_wanted(self, url, tag): devuelve booleano



    def preprocess_raw_html(self, raw_html, url):

        return '<html><head></head><body>'+raw_html[raw_html.find('<h1>'):]

    def preprocess_html(self, soup):

        title = soup.h1
        volanta = soup.p
        #pendiente sibling de volanta
        columnista = soup.find(name=['div'], attrs={'class':lambda x: x and 'columnista' in x.split()})

        caca = columnista.ul
        if caca:
            caca.name = "p"
            for each in caca.findAll(name=['li']):
                each.name == "span"

        nota = soup.find(name=['div'], attrs={'class':'nota'})
        imagen = soup.find(name=['div'], attrs={'class':'img-box'})

        mysoup = self.index_to_soup("<html><head></head><body></body></html>")
        mysoup.body.insert(len(mysoup.body), title)
        mysoup.body.insert(len(mysoup.body), volanta)
        if imagen:
            mysoup.body.insert(len(mysoup.body), imagen)
        if columnista:
            mysoup.body.insert(len(mysoup.body), columnista)
        mysoup.body.insert(len(mysoup.body), nota)
        open('/home/user/Escritorio/recipe_test/preprocess_html.txt', 'wb').write(unicode(mysoup))

        return mysoup

    def postprocess_html(self, soup, first_fetch):
        '''
        This method is called with the source of each downloaded :term:`HTML` file, after
        it is parsed for links and images.
        It can be used to do arbitrarily powerful post-processing on the :term:`HTML`.
        It should return `soup` after processing it.

        :param soup: A `BeautifulSoup <http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html>`_  instance containing the downloaded :term:`HTML`.
        :param first_fetch: True if this is the first page of an article.

        '''

        soup.find(name=['div'], attrs={'class':lambda x: x and 'calibre_navbar' in x.split()}).extract()
        open('/home/user/Escritorio/recipe_test/postprocess_html.txt', 'wb').write(unicode(soup))
        return soup
